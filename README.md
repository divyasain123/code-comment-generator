# ğŸ¤– Code Comment Generator with Transformers

## ğŸ“‹ Project Overview

**Complete Transformer implementation from scratch for automatic Python code documentation**

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.0+-green.svg)](https://streamlit.io)

> *Built from scratch following "Attention Is All You Need" - demonstrates complete ML engineering pipeline and critical evaluation of model limitations.*

## ğŸ—ï¸ Technical Architecture

### Transformer Model (1.39M Parameters)

* ğŸ”„ **4 Encoder + 4 Decoder layers** with **8-head self-attention**
* ğŸ§  **Custom tokenizer** with 181-token vocabulary (built from scratch)
* âš™ï¸ Fully from scratch implementation of:

  * Multi-head attention
  * Scaled dot-product attention
  * Positional encoding
  * Layer normalization
  * Residual connections

### Training Results

* ğŸ”» **Loss reduced from 5.24 â†’ 1.65** over 90 epochs
* ğŸ§ª Trained on 15 curated high-quality Python code-comment pairs

---

## ğŸ“ Project Structure

```
src/
â”œâ”€â”€ models/              # Transformer model implementation
â”‚   â”œâ”€â”€ attention.py     # Multi-head attention
â”‚   â”œâ”€â”€ encoder.py       # Encoder layers
â”‚   â”œâ”€â”€ decoder.py       # Decoder layers
â”‚   â””â”€â”€ transformer.py   # Combined Transformer model
â”œâ”€â”€ data_processing/     # Tokenization, dataset utilities
â”œâ”€â”€ training/            # Training loop, checkpoints, early stopping
â””â”€â”€ inference/           # Streamlit interface + model loading
```

---

## ğŸš§ Challenges & Solutions

### ğŸ§© Challenge 1: Data Scarcity & Collection

* **Problem**: GitHub API rate limits + parsing complexity
* **Solution**: Hand-curated 15 diverse examples to demonstrate concept efficacy

### âš™ï¸ Challenge 2: Attention Mechanism Complexity

* **Problem**: Implementing and debugging tensor shapes
* **Solution**: Built modular attention layers with careful broadcasting

```python
def scaled_dot_product_attention(Q, K, V, mask=None):
    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    weights = F.softmax(scores, dim=-1)
    return torch.matmul(weights, V)
```

### ğŸ§ª Challenge 3: Overfitting

* **Problem**: High training accuracy but poor generalization
* **Solution**: Early stopping, regularization, gradient clipping

---

## ğŸ“Š Results & Insights

| Metric          | Value                                  |
| --------------- | -------------------------------------- |
| Training Loss   | 5.24 â†’ 1.65                            |
| Validation Loss | \~4.6 (expected due to low data)       |
| Strength        | Works well on simple utility functions |
| Limitation      | Fails on complex multi-branch logic    |

> ğŸ’¡ **Key Insight**: Model complexity isn't a substitute for good data. High-quality, diverse training data is vital.

---

## ğŸ’¼ Real-World Learnings

| Limitation                   | Reason               | Suggested Fix                                |
| ---------------------------- | -------------------- | -------------------------------------------- |
| Poor complex code generation | Low data             | Use CodeBERT or CodeT5 for transfer learning |
| Overfit training set         | Small dataset        | Automate collection from GitHub repos        |
| Output quality               | Narrow code patterns | Augment dataset, filter noisy examples       |

---

## ğŸ’» Quickstart

### ğŸ”§ Setup

```bash
git clone https://github.com/VarenyaVisen/code-comment-generator.git
cd code-comment-generator
pip install -r requirements.txt
streamlit run src/inference/streamlit_app.py
```

---

## ğŸ› ï¸ Tech Stack

* **Python** - Core language
* **PyTorch** - Model training
* **Streamlit** - Web interface
* **NumPy, Matplotlib** - Utilities & debugging
* **Custom Tokenizer** - Built from scratch

---

## ğŸ“ Skills Demonstrated

* âœ… Deep Learning: Transformer internals & attention math
* âœ… ML Engineering: Training loops, monitoring, checkpoints
* âœ… Web Dev: Deployable interface using Streamlit
* âœ… Software Design: Modular, reusable architecture
* âœ… Honest Evaluation: Understanding when models fail

---

## ğŸ”® Future Improvements

* ğŸ” **Expand Dataset**: Scrape and clean 1K+ GitHub examples for better generalization
* ğŸ“ˆ **Model Optimization**: Try distilled transformer variants to reduce inference time
* ğŸ§  **Pretrained Code Models**: Fine-tune CodeBERT or CodeT5 for improved performance
* ğŸŒ **VSCode Plugin**: Wrap model into a local extension for real-time comment suggestions
* ğŸš€ **REST API Deployment**: Dockerize and serve via FastAPI or Flask for broader integration

---

---

â­ *Star this repo if you found it helpful*!
*Built from scratch with â¤ï¸ and curiosity for deep learning + real-world ML!*
